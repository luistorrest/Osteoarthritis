{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","        break\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-01-22T06:32:11.844315Z","iopub.status.busy":"2023-01-22T06:32:11.843459Z","iopub.status.idle":"2023-01-22T06:32:48.924836Z","shell.execute_reply":"2023-01-22T06:32:48.923846Z","shell.execute_reply.started":"2023-01-22T06:32:11.844225Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["La sintaxis del comando no es correcta.\n","El sistema no puede encontrar la ruta especificada.\n","La sintaxis del comando no es correcta.\n","\"cp\" no se reconoce como un comando interno o externo,\n","programa o archivo por lotes ejecutable.\n","\"cp\" no se reconoce como un comando interno o externo,\n","programa o archivo por lotes ejecutable.\n","\"cp\" no se reconoce como un comando interno o externo,\n","programa o archivo por lotes ejecutable.\n","\"cp\" no se reconoce como un comando interno o externo,\n","programa o archivo por lotes ejecutable.\n","\"cp\" no se reconoce como un comando interno o externo,\n","programa o archivo por lotes ejecutable.\n","\"cp\" no se reconoce como un comando interno o externo,\n","programa o archivo por lotes ejecutable.\n"]},{"ename":"ModuleNotFoundError","evalue":"No module named 'cv2'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp -r /kaggle/input/osteoarthritis-prediction/Valid/Valid/Osteoarthritis/*.png Osteoarthritis/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mcv2\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mrandom\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"]}],"source":["!mkdir tmp/\n","!cd tmp/\n","!mkdir Normal/ Osteoarthritis/\n","!cp -r /kaggle/input/osteoarthritis-prediction/train/train/Normal/*.png Normal/\n","!cp -r /kaggle/input/osteoarthritis-prediction/train/train/Osteoarthritis/*.png Osteoarthritis/\n","!cp -r /kaggle/input/osteoarthritis-prediction/test/test/Normal/*.png Normal/\n","!cp -r /kaggle/input/osteoarthritis-prediction/test/test/Osteoarthritis/*.png Osteoarthritis/\n","!cp -r /kaggle/input/osteoarthritis-prediction/Valid/Valid/Normal/*.png Normal/\n","!cp -r /kaggle/input/osteoarthritis-prediction/Valid/Valid/Osteoarthritis/*.png Osteoarthritis/\n","\n","\n","import torch\n","import os, sys, json, cv2, random, torchvision\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchvision import transforms\n","import seaborn as sns\n","from PIL import Image\n","from numpy import interp\n","import warnings\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm\n","from sklearn.metrics import auc, f1_score, roc_curve, classification_report, confusion_matrix\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from itertools import cycle"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-22T06:33:47.640824Z","iopub.status.busy":"2023-01-22T06:33:47.63985Z","iopub.status.idle":"2023-01-22T06:33:49.721822Z","shell.execute_reply":"2023-01-22T06:33:49.720474Z","shell.execute_reply.started":"2023-01-22T06:33:47.640785Z"},"trusted":true},"outputs":[],"source":["!cp -r Normal/ tmp/\n","!cp -r Osteoarthritis/ tmp/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-22T06:38:59.561217Z","iopub.status.busy":"2023-01-22T06:38:59.560802Z","iopub.status.idle":"2023-01-22T06:40:50.654755Z","shell.execute_reply":"2023-01-22T06:40:50.653513Z","shell.execute_reply.started":"2023-01-22T06:38:59.561187Z"},"trusted":true},"outputs":[],"source":["class NeuralNetwork():\n","\n","    def __init__(self, root='tmp/', epochs=5, batch_size=32, learning_rate=0.0003, model_name=None, plot_roc=True,\n","                 predictor=True, plot_acc_loss=True, best_val_acc=0, save_path='./Weights', use_multi_gpus=False,\n","                 plot_image_class_distribution=True, save_name='./Weights/model.pth', csv_path='./ValAccuracy.csv'):\n","\n","        \"\"\"\n","        root: Your image_data path\n","        epochs: Training_epochs (default 5)\n","        batch_size: Batch_size [default 32 (if you want to change the batch_size, you also need change the learning_rate)]\n","        learning_rate: Optimizer learning_rate (default 0.01)\n","        model_name: Choose your model {'ResNet50', 'ResNet101', 'ResNet152'}(default None: if None, use 'ResNet50')\n","        predictor: Do you need predict and plot confusion_matrix? (default False)\n","        plot_acc_loss: Do you need plot train_accuracy train_loss val_accuracy val_loss? (default False)\n","        best_val_acc: Initial the best val accuracy for comparing with val_accuracy every epoch (default 0)\n","        save_path: The directory for save model weights (default './Weights')\n","        use_multi_gpus: Do you want to use multi_gpus? (default False)\n","        plot_image_class_distribution: Do you want to plot every classes distribution in a bar_plot? (default False)\n","        save_name: Your model_weight's name (default './Weights/model.pth')\n","        csv_path: Create a csv file to write accuracy and loss for every epoch (default './ValAccuracy.csv')\n","        \"\"\"\n","\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        self.plot_roc = plot_roc\n","\n","        if use_multi_gpus:\n","            assert torch.cuda.device_count() > 1, 'You must be have more than one gpu'\n","            self.devices = [\n","                torch.device(i) for i in range(torch.cuda.device_count())\n","            ]\n","\n","        self.model_dict = {\n","            'ResNet50': [3, 4, 6, 3],\n","            'ResNet101': [3, 4, 23, 3],\n","            'ResNet152': [3, 4, 36, 3]\n","        }\n","\n","        self.plot_acc_loss = plot_acc_loss\n","        self.predictor = predictor\n","        self.lr = learning_rate\n","        self.use_multi_gpus = use_multi_gpus\n","        self.root = root\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.plot_image_class_distribution = plot_image_class_distribution\n","        self.save_path = save_path\n","        self.save_name = save_name\n","        self.model_name = model_name\n","        self.csv_path = csv_path\n","        self.best_val_acc = best_val_acc\n","\n","        self.data_transform = {\n","            'train': transforms.Compose([transforms.RandomResizedCrop(224), transforms.ToTensor(),\n","                                         transforms.RandomHorizontalFlip(),\n","                                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n","            'valid': transforms.Compose([transforms.Resize((224, 224)), transforms.CenterCrop(224),\n","                                         transforms.ToTensor(),\n","                                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n","        }\n","\n","    @staticmethod\n","    def read_split_data(root, plot_image=False):\n","        filepaths = []\n","        labels = []\n","        bad_images = []\n","\n","        random.seed(0)\n","        assert os.path.exists(root), f'Your data_path: {root} is wrong!'\n","\n","        classes = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))]\n","        classes.sort()\n","        class_indices = {k: v for v, k in enumerate(classes)}\n","\n","        json_str = json.dumps({v: k for k, v in class_indices.items()}, indent=4)\n","\n","        with open('./classes_indices.json', 'w') as json_file:\n","            json_file.write(json_str)\n","\n","        every_class_num = []\n","        supported = ['.jpg', '.png', '.jpeg', '.PNG', '.JPG', '.JPEG']\n","        for klass in classes:\n","            classpath = os.path.join(root, klass)\n","            images = [os.path.join(root, klass, i) for i in os.listdir(classpath) if\n","                      os.path.splitext(i)[-1] in supported]\n","            every_class_num.append(len(images))\n","            flist = sorted(os.listdir(classpath))\n","            desc = f'{klass:23s}'\n","            for f in tqdm(flist, ncols=110, desc=desc, unit='file', colour='blue'):\n","                fpath = os.path.join(classpath, f)\n","                fl = f.lower()\n","                index = fl.rfind('.')\n","                ext = fl[index:]\n","                if ext in supported:\n","                    try:\n","                        img = cv2.imread(fpath)\n","                        filepaths.append(fpath)\n","                        labels.append(klass)\n","                    except:\n","                        bad_images.append(fpath)\n","                        print('defective image file: ', fpath)\n","                else:\n","                    bad_images.append(fpath)\n","\n","        Fseries = pd.Series(filepaths, name='filepaths')\n","        Lseries = pd.Series(labels, name='labels')\n","        df = pd.concat([Fseries, Lseries], axis=1)\n","\n","        print(f'{len(df.labels.unique())} kind of images were found in the dataset')\n","        train_df, test_df = train_test_split(df, train_size=.8, shuffle=True, random_state=123, stratify=df['labels'])\n","\n","        train_image_path = train_df['filepaths'].tolist()\n","        val_image_path = test_df['filepaths'].tolist()\n","\n","        train_image_label = [class_indices[i] for i in train_df['labels'].tolist()]\n","        val_image_label = [class_indices[i] for i in test_df['labels'].tolist()]\n","\n","        sample_df = train_df.sample(n=50, replace=False)\n","        ht, wt, count = 0, 0, 0\n","        for i in range(len(sample_df)):\n","            fpath = sample_df['filepaths'].iloc[i]\n","            try:\n","                img = cv2.imread(fpath)\n","                h = img.shape[0]\n","                w = img.shape[1]\n","                ht += h\n","                wt += w\n","                count += 1\n","            except:\n","                pass\n","\n","        have = int(ht / count)\n","        wave = int(wt / count)\n","        aspect_ratio = have / wave\n","        print('{} images were found in the dataset.\\n{} for training, {} for validation'.format(\n","            sum(every_class_num), len(train_image_path), len(val_image_path)\n","        ))\n","        print('average image height= ', have, '  average image width= ', wave, ' aspect ratio h/w= ', aspect_ratio)\n","\n","        if plot_image:\n","            plt.bar(range(len(classes)), every_class_num, align='center')\n","            plt.xticks(range(len(classes)), classes)\n","\n","            for i, v in enumerate(every_class_num):\n","                plt.text(x=i, y=v + 5, s=str(v), ha='center')\n","\n","            plt.xlabel('image class')\n","            plt.ylabel('number of images')\n","\n","            plt.title('class distribution')\n","            plt.show()\n","        return train_image_path, train_image_label, val_image_path, val_image_label, class_indices\n","\n","    def train_step(self, net, optimizer, data_loader, device, epoch):\n","        net.train()\n","        loss_function = nn.CrossEntropyLoss()\n","        train_acc, train_loss, sampleNum = 0, 0, 0\n","        optimizer.zero_grad()\n","\n","        train_bar = tqdm(data_loader, file=sys.stdout, colour='red')\n","        for step, data in enumerate(train_bar):\n","            images, labels = data\n","            sampleNum += images.shape[0]\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = net(images)\n","            loss = loss_function(outputs, labels)\n","            train_acc += (torch.argmax(outputs, dim=1) == labels).sum().item()\n","            train_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","            train_bar.desc = \"[train epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch, train_loss / (step + 1),\n","                                                                                 train_acc / sampleNum)\n","        return round(train_loss / (step + 1), 3), round(train_acc / sampleNum, 3)\n","\n","    @torch.no_grad()\n","    def val_step(self, net, data_loader, device, epoch):\n","        loss_function = nn.CrossEntropyLoss()\n","        net.eval()\n","        val_acc = 0\n","        val_loss = 0\n","        sample_num = 0\n","        val_bar = tqdm(data_loader, file=sys.stdout, colour='red')\n","        for step, data in enumerate(val_bar):\n","            images, labels = data\n","            sample_num += images.shape[0]\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = net(images)\n","            loss = loss_function(outputs, labels)\n","            val_loss += loss.item()\n","            val_acc += (torch.argmax(outputs, dim=1) == labels).sum().item()\n","            val_bar.desc = \"[valid epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch, val_loss / (step + 1),\n","                                                                               val_acc / sample_num)\n","        return round(val_loss / (step + 1), 3), round(val_acc / sample_num, 3)\n","\n","    def fit(self):\n","        train_image_path, train_image_label, val_image_path, val_image_label, class_indices = self.read_split_data(\n","            self.root, self.plot_image_class_distribution)\n","\n","        if not os.path.exists(self.save_path):\n","            os.makedirs(self.save_path)\n","\n","        if os.path.exists(self.csv_path):\n","            self.best_val_acc = max(pd.read_csv(self.csv_path)['Val_Accuracy'].tolist())\n","            df = pd.DataFrame(index=list(range(self.epochs)),\n","                              columns=['Epochs', 'Train_Loss', 'Train_Accuracy', 'Val_Loss', 'Val_Accuracy'])\n","        else:\n","            df = pd.DataFrame(index=list(range(self.epochs)),\n","                              columns=['Epochs', 'Train_Loss', 'Train_Accuracy', 'Val_Loss', 'Val_Accuracy'])\n","\n","        # ResMLP Block\n","        class ResidualMLPBlock(nn.Module):\n","            def __init__(self, input_units, output_units, residual_path) -> None:\n","                super().__init__()\n","\n","                self.residual_path = residual_path\n","\n","                self.features_1 = nn.Sequential(\n","                    nn.Linear(input_units, output_units), nn.ReLU(),\n","                    nn.Linear(output_units, output_units), nn.ReLU()\n","                )\n","\n","                if residual_path:\n","                    self.features_2 = nn.Sequential(\n","                        nn.Linear(input_units, output_units), nn.GELU(),\n","                        nn.Linear(output_units, output_units), nn.GELU()\n","                    )\n","\n","            def forward(self, x):\n","                residual = x\n","                y = self.features_1(x)\n","                if self.residual_path:\n","                    residual = self.features_2(x)\n","                output = y + residual\n","                return output\n","\n","        # 残差MLP 特征融合 鲁棒性更好 相对于简单MLP来说不易过拟合\n","        class ResMLP(nn.Module):\n","            def __init__(self, initial_units, block_list, num_classes, initial_weights=True) -> None:\n","                super().__init__()\n","\n","                self.initial_units = initial_units\n","                self.block_list = block_list\n","\n","                self.layer1 = nn.Sequential(\n","                    nn.Linear(initial_units, 32),\n","                    nn.BatchNorm1d(num_features=32),\n","                    nn.ReLU(inplace=True)\n","                )\n","\n","                self.layer2 = self._make_layer(block_num=self.block_list[0], input_units=32, output_units=64)\n","                self.layer3 = self._make_layer(block_num=self.block_list[1], input_units=64, output_units=128)\n","                self.layer4 = self._make_layer(block_num=self.block_list[2], input_units=128, output_units=256)\n","                self.layer5 = self._make_layer(block_num=self.block_list[3], input_units=256, output_units=512)\n","\n","                self.out = nn.Sequential(\n","                    nn.Dropout(0.5),\n","                    nn.Linear(512, num_classes)\n","                )\n","\n","                if initial_weights:\n","                    self._initialize_weights()\n","\n","            def _make_layer(self, block_num, input_units, output_units):\n","                blocks = []\n","                blocks.append(ResidualMLPBlock(input_units, output_units, residual_path=True))\n","                for _ in range(1, block_num):\n","                    blocks.append(ResidualMLPBlock(output_units, output_units, residual_path=False))\n","\n","                return nn.Sequential(*blocks)\n","\n","            def _initialize_weights(self):\n","                for m in self.modules():\n","                    if isinstance(m, nn.Linear):\n","                        nn.init.normal_(m.weight, 0, 0.01)\n","                        nn.init.constant_(m.bias, 0)\n","\n","            def forward(self, x):\n","                x = self.layer1(x)\n","                x = self.layer2(x)\n","                x = self.layer3(x)\n","                x = self.layer4(x)\n","                x = self.layer5(x)\n","                y = self.out(x)\n","                return y\n","\n","        # ResNet50+ResMLP 引入30层残差MLP 一共80层网络 泛化性比Res50要好 F1-score分数更高 不易过拟合 可根据实际情况使用\n","        class ResNet(nn.Module):\n","            def __init__(self, num_classes) -> None:\n","                super().__init__()\n","\n","                self.features = torchvision.models.resnet50(pretrained=True)\n","                self.classifier = ResMLP(initial_units=1000, block_list=[2, 2, 2, 2], num_classes=num_classes)\n","\n","                for m in self.modules():\n","                    if isinstance(m, nn.Linear):\n","                        nn.init.xavier_normal_(m.weight)\n","                        nn.init.constant_(m.bias, 0)\n","\n","            def forward(self, x):\n","                x = self.features(x)\n","                y = self.classifier(x)\n","                return y\n","\n","        class MyDataset(Dataset):\n","            def __init__(self, image_path, image_labels, transform=None):\n","                self.image_path = image_path\n","                self.image_class = image_labels\n","                self.transform = transform\n","\n","            def __len__(self):\n","                return len(self.image_path)\n","\n","            def __getitem__(self, item):\n","                # 转成RGB格式\n","                img = Image.open(self.image_path[item]).convert('RGB')\n","                label = self.image_class[item]\n","                if self.transform is not None:\n","                    img = self.transform(img)\n","                return img, label\n","\n","            # DataLoader 方法会用到 如果不设置 则使用官方默认的\n","            @staticmethod\n","            def collate_fn(batch):\n","                images, labels = tuple(zip(*batch))\n","                images = torch.stack(images, dim=0)\n","                labels = torch.as_tensor(labels)\n","                return images, labels\n","\n","        train_dataset = MyDataset(train_image_path, train_image_label, self.data_transform['train'])\n","        valid_dataset = MyDataset(val_image_path, val_image_label, self.data_transform['valid'])\n","\n","        # Linux 环境下可以开启多线程 num_workers\n","        system_name = sys.platform\n","        nw = min([os.cpu_count(), self.batch_size if self.batch_size > 1 else 0, 8]) if 'linux' in system_name else 0\n","        train_loader = DataLoader(train_dataset, shuffle=True, batch_size=self.batch_size, pin_memory=True,\n","                                  num_workers=nw, collate_fn=train_dataset.collate_fn)\n","        valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=self.batch_size, pin_memory=True,\n","                                  num_workers=nw, collate_fn=valid_dataset.collate_fn)\n","\n","        if self.model_name:\n","            net = ResNet(num_classes=2)\n","        else:\n","            self.model_name = 'ResNet50'\n","            net = ResNet(num_classes=2)\n","\n","        if self.use_multi_gpus:\n","            net = nn.DataParallel(net, device_ids=self.devices).to(self.devices[0])\n","        else:\n","            net = net.to(self.device)\n","\n","        # summary(net, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\",\n","        #             \"num_params\", \"kernel_size\", \"mult_adds\", \"trainable\"])\n","\n","        optimizer = torch.optim.Adam(net.parameters(), lr=self.lr)\n","        train_Loss, train_Accuracy, val_Loss, val_Accuracy = [], [], [], []\n","\n","        tb_writer = SummaryWriter()\n","        scheduler = CosineAnnealingLR(optimizer, T_max=1)\n","        for epoch in range(self.epochs):\n","            train_loss, train_accuracy = self.train_step(net, optimizer, train_loader,\n","                                                         self.devices[0] if self.use_multi_gpus else self.device, epoch)\n","            val_loss, val_accuracy = self.val_step(net, valid_loader,\n","                                                   self.devices[0] if self.use_multi_gpus else self.device, epoch)\n","            scheduler.step()\n","\n","            train_Loss.append(train_loss)\n","            train_Accuracy.append(train_accuracy)\n","            val_Loss.append(val_loss)\n","            val_Accuracy.append(val_accuracy)\n","\n","            # Tensorboard Plot\n","            tags = [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"learning_rate\"]\n","\n","            tb_writer.add_scalar(tags[0], train_loss, epoch)\n","            tb_writer.add_scalar(tags[1], train_accuracy, epoch)\n","            tb_writer.add_scalar(tags[2], val_loss, epoch)\n","            tb_writer.add_scalar(tags[3], val_accuracy, epoch)\n","            tb_writer.add_scalar(tags[4], optimizer.param_groups[0][\"lr\"], epoch)\n","\n","            if val_accuracy > self.best_val_acc:\n","                self.best_val_acc = val_accuracy\n","                torch.save(net.state_dict(), self.save_name)\n","\n","        # 'Train_Loss', 'Train_Accuracy', 'Val_Loss', 'Val_Accuracy'\n","        df['Epochs'] = list(range(self.epochs))\n","        df['Train_Loss'] = train_Loss\n","        df['Train_Accuracy'] = train_Accuracy\n","        df['Val_Loss'] = val_Loss\n","        df['Val_Accuracy'] = val_Accuracy\n","\n","        df.to_csv(self.csv_path, index=False)\n","\n","        if self.plot_acc_loss:\n","            self.Plot_Acc_Loss(df)\n","\n","        if self.predictor:\n","            self.Predictor(net, valid_loader)\n","\n","        if self.plot_roc:\n","            self.Plot_ROC(net, valid_loader)\n","\n","\n","    def Plot_ROC(self, net, val_loader):\n","\n","        try:\n","            json_file = open('./classes_indices.json', 'r')\n","            class_indict = json.load(json_file)\n","        except Exception as e:\n","            print(e)\n","            exit(-1)\n","\n","        score_list = []  # 存储预测得分\n","        label_list = []  # 存储真实标签\n","\n","        net.load_state_dict(torch.load(self.save_name))\n","\n","        for i, data in enumerate(val_loader):\n","            images, labels = data\n","            images, labels = images.to(self.device), labels.to(self.device)\n","            outputs = torch.softmax(net(images), dim=0)\n","            score_tmp = outputs\n","            score_list.extend(score_tmp.detach().cpu().numpy())\n","            label_list.extend(labels.cpu().numpy())\n","\n","        score_array = np.array(score_list)\n","        # 将label转换成onehot形式\n","        label_tensor = torch.tensor(label_list)\n","        label_tensor = label_tensor.reshape((label_tensor.shape[0], 1))\n","        label_onehot = torch.zeros(label_tensor.shape[0], len(class_indict.keys()))\n","        label_onehot.scatter_(dim=1, index=label_tensor, value=1)\n","        label_onehot = np.array(label_onehot)\n","\n","        print(\"score_array:\", score_array.shape)  # (batchsize, classnum)\n","        print(\"label_onehot:\", label_onehot.shape)  # torch.Size([batchsize, classnum])\n","\n","        # 调用sklearn库，计算每个类别对应的fpr和tpr\n","        fpr_dict = dict()\n","        tpr_dict = dict()\n","        roc_auc_dict = dict()\n","        for i in range(len(class_indict.keys())):\n","            fpr_dict[i], tpr_dict[i], _ = roc_curve(label_onehot[:, i], score_array[:, i])\n","            roc_auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])\n","        # micro\n","        fpr_dict[\"micro\"], tpr_dict[\"micro\"], _ = roc_curve(label_onehot.ravel(), score_array.ravel())\n","        roc_auc_dict[\"micro\"] = auc(fpr_dict[\"micro\"], tpr_dict[\"micro\"])\n","\n","        # macro\n","        # First aggregate all false positive rates\n","        all_fpr = np.unique(np.concatenate([fpr_dict[i] for i in range(len(class_indict.keys()))]))\n","        # Then interpolate all ROC curves at this points\n","        mean_tpr = np.zeros_like(all_fpr)\n","\n","        for i in range(len(set(label_list))):\n","            mean_tpr += interp(all_fpr, fpr_dict[i], tpr_dict[i])\n","\n","        # Finally average it and compute AUC\n","        mean_tpr /= len(class_indict.keys())\n","        fpr_dict[\"macro\"] = all_fpr\n","        tpr_dict[\"macro\"] = mean_tpr\n","        roc_auc_dict[\"macro\"] = auc(fpr_dict[\"macro\"], tpr_dict[\"macro\"])\n","\n","        # 绘制所有类别平均的roc曲线\n","        plt.figure(figsize=(8, 8))\n","        lw = 2\n","        plt.plot(fpr_dict[\"micro\"], tpr_dict[\"micro\"],\n","                 label='micro-average ROC curve (area = {0:0.2f})'\n","                       ''.format(roc_auc_dict[\"micro\"]),\n","                 color='deeppink', linestyle=':', linewidth=4)\n","\n","        plt.plot(fpr_dict[\"macro\"], tpr_dict[\"macro\"],\n","                 label='macro-average ROC curve (area = {0:0.2f})'\n","                       ''.format(roc_auc_dict[\"macro\"]),\n","                 color='navy', linestyle=':', linewidth=4)\n","\n","        colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n","        for i, color in zip(range(len(class_indict.keys())), colors):\n","            plt.plot(fpr_dict[i], tpr_dict[i], color=color, lw=lw,\n","                     label='ROC curve of class {0} (area = {1:0.2f})'\n","                           ''.format(class_indict[str(i)], roc_auc_dict[i]))\n","\n","        plt.plot([0, 1], [0, 1], 'k--', lw=lw, label='Chance', color='red')\n","        plt.xlim([0.0, 1.0])\n","        plt.ylim([0.0, 1.05])\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.title('Receiver operating characteristic to multi-class')\n","        plt.legend(loc=\"lower right\")\n","        # plt.savefig('set113_roc.jpg')\n","        plt.show()\n","\n","\n","    def Plot_Acc_Loss(self, df):\n","        Epochs = df.loc[:, 'Epochs'].tolist()\n","        tloss = df.loc[:, 'Train_Loss'].tolist()\n","        vloss = df.loc[:, 'Val_Loss'].tolist()\n","        tacc = df.loc[:, 'Train_Accuracy'].tolist()\n","        vacc = df.loc[:, 'Val_Accuracy'].tolist()\n","\n","        plt.style.use('fivethirtyeight')\n","        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n","        axes[0].plot(Epochs, tloss, 'r', label='Training loss')\n","        axes[0].plot(Epochs, vloss, 'g', label='Validation loss')\n","        axes[0].set_title('Training and Validation Loss')\n","        axes[0].set_xlabel('Epochs', fontsize=18)\n","        axes[0].set_ylabel('Loss', fontsize=18)\n","        axes[0].legend()\n","\n","        axes[1].plot(Epochs, tacc, 'r', label='Training Accuracy')\n","        axes[1].plot(Epochs, vacc, 'g', label='Validation Accuracy')\n","        axes[1].set_title('Training and Validation Accuracy')\n","        axes[1].set_xlabel('Epochs', fontsize=18)\n","        axes[1].set_ylabel('Accuracy', fontsize=18)\n","        axes[1].legend()\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    def Predictor(self, net, test_loader):\n","        try:\n","            json_file = open('./classes_indices.json', 'r')\n","            class_indict = json.load(json_file)\n","        except Exception as e:\n","            print(e)\n","            exit(-1)\n","\n","        errors = 0\n","        y_pred, y_true = [], []\n","        net.load_state_dict(torch.load(self.save_name))\n","\n","        net.eval()\n","        with torch.no_grad():\n","            for data in test_loader:\n","                images, labels = data\n","                images, labels = images.to(self.device), labels.to(self.device)\n","                preds = torch.argmax(torch.softmax(net(images), dim=0), dim=1)\n","                for i in range(len(preds)):\n","                    y_pred.append(preds[i].cpu())\n","                    y_true.append(labels[i].cpu())\n","\n","        tests = len(y_pred)\n","        for i in range(tests):\n","            pred_index = y_pred[i]\n","            true_index = y_true[i]\n","            if pred_index != true_index:\n","                errors += 1\n","        acc = (1 - errors / tests) * 100\n","        print(f'there were {errors} errors in {tests} tests for an accuracy of {acc:6.2f}%')\n","\n","        ypred = np.array(y_pred)\n","        ytrue = np.array(y_true)\n","\n","        f1score = f1_score(ytrue, ypred, average='weighted') * 100\n","\n","        print(f'The F1-score was {f1score:.3f}')\n","        class_count = len(list(class_indict.values()))\n","        classes = list(class_indict.values())\n","\n","        cm = confusion_matrix(ytrue, ypred)\n","        plt.figure(figsize=(16, 8))\n","        plt.subplot(1, 2, 1)\n","        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)\n","        plt.xticks(np.arange(class_count) + .5, classes, rotation=90, fontsize=14)\n","        plt.yticks(np.arange(class_count) + .5, classes, rotation=0, fontsize=14)\n","        plt.xlabel(\"Predicted\", fontsize=14)\n","        plt.ylabel(\"Actual\", fontsize=14)\n","        plt.title(\"Confusion Matrix\")\n","\n","        plt.subplot(1, 2, 2)\n","        sns.heatmap(cm / np.sum(cm), annot=True, fmt='.1%')\n","        plt.xlabel('Predicted Label', fontsize=14)\n","        plt.xticks(fontsize=14)\n","        plt.ylabel('True Label', fontsize=14)\n","        plt.yticks(fontsize=14)\n","        plt.show()\n","\n","        clr = classification_report(y_true, y_pred, target_names=classes, digits=4)\n","        print(\"Classification Report:\\n----------------------\\n\", clr)\n","\n","\n","if __name__ == '__main__':\n","    model = NeuralNetwork()\n","    model.fit()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2803840,"sourceId":4838250,"sourceType":"datasetVersion"}],"dockerImageVersionId":30357,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
